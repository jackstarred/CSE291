{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 : Math for Robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Ruffin White  \n",
    "Course: CSE291  \n",
    "Date: Mar 9 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Make inline plots vector graphics instead of raster graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('pdf', 'svg')\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "# set_matplotlib_formats('png')\n",
    "\n",
    "# import modules for plotting and data analysis\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "\n",
    "> Use the ATT dataset http://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att faces.zip. to compute subspaces for the PCA and LDA methods. Provide illustration of the respective 1st, 2nd and 3rd eigenvectors. Compute the recognition rates for the test-set. Report\n",
    "\n",
    "* Correct classification\n",
    "* Incorrect classification\n",
    "\n",
    "> Provide at least one suggestion for how you might improve performance of the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import some common numerical libraries, as well as some image, data structure libraries for manipulating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll add a helper function to recursively search for the face images in the dataset, load them into flatten feature vectors, then interleave them into a dataframe with labels ascertained from the originating directory name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_dir_path):\n",
    "    rows = []\n",
    "    height, width = 112, 92\n",
    "    flattened = height * width # 10304\n",
    "    X_keys = [\"X_\" + str(i) for i in range(flattened)]\n",
    "    paths = pathlib.Path(data_dir_path).glob('**/*.pgm')\n",
    "    for path in paths:\n",
    "        im = Image.open(str(path))\n",
    "        X_values = np.array(im).flatten()\n",
    "        y_value = path.parent.name\n",
    "        row = OrderedDict(zip(X_keys, X_values))\n",
    "        row['y'] = y_value\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read in the data and split the feature vectors and labels into separate yet commonly indexed dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir_path = 'data/att_faces/'\n",
    "\n",
    "df = read_data(data_dir_path)\n",
    "X = df.drop('y',axis=1)\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the integrity of the parsed data by recalling the original shape of the images to reshape the flattened features back into the pixelated images they encode, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "numbers = [0,1,398,399]\n",
    "for i in range(4):\n",
    "    im = X.loc[numbers[i]].values.reshape(112, 92)\n",
    "    label = y.loc[numbers[i]]\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(label)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can also Standardize the feature vectors. Whether we should standardize the data prior to a PCA depends on the measurement scales of the original features. PCA yields a feature subspace that maximizes the variance along found axes, so it makes sense to standardize the data, especially, if it was measured on different scales (which isn't necessarily the case here given all pixel values fall within a common range). \n",
    "\n",
    "This will transform the data onto unit scale (mean=0 and variance=1), which is a common requirement for the optimal performance of many machine learning algorithms. This should also heighten the sensitivity to pixels that change only subtly thought the hole dataset. We will see if this is to our classifiers benefit or detriment later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.keys())\n",
    "# X_std.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For comparision, the standerdized data is also visualed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "numbers = [0,1,398,399]\n",
    "for i in range(4):\n",
    "    im = X_std.loc[numbers[i]].values.reshape(112, 92)\n",
    "    label = y.loc[numbers[i]]\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(label)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now separate our data for train and testing in an 80:20 split, careful to make sure stratify over our labels to ensure all classes are evenaly represented in both training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.as_matrix(), y.as_matrix(),\n",
    "                                                    test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "X_std_train, X_std_test, y_std_train, y_std_test = train_test_split(X_std.as_matrix(), y.as_matrix(),\n",
    "                                                    test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit a PCA model to both our original and standardized training data to determine the dominant eigenvectors for our PCA subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=X_train.shape[1])\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print('X_std_pca.shape:', X_train_pca.shape)\n",
    "print('pca.components_:', pca.components_.shape)\n",
    "\n",
    "pca_std = PCA(n_components=X_std_train.shape[1])\n",
    "pca_std.fit(X_std_train)\n",
    "X_std_train_pca = pca_std.transform(X_std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By extracting the eigenvectors from the fitted model, we can see what they look like in 2D by again reshaping the vecorts for pixel imagery. Below are the eigenvectors for the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "for i in range(20):\n",
    "    eigenvector = pca.components_[i].reshape(112, 92)\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(eigenvector, cmap='gray')\n",
    "    plt.title(\"Eigenvector: {0}\".format(i+1))\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below here we see the eigenvectors from the standardized data. Not much of difference for the first few dominate vectors other than shifted bias in grayscale, but smaller vectors seem swap around quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "for i in range(20):\n",
    "    eigenvector = pca_std.components_[i].reshape(112, 92)\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(eigenvector, cmap='gray')\n",
    "    plt.title(\"Eigenvector: {0}\".format(i+1))\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can fit an LDA model on to the original training data a view the coefficients learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "for i in range(20):\n",
    "    eigenvector = lda.coef_[i].reshape(112, 92)\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(eigenvector, cmap='gray')\n",
    "    plt.title(\"Coefficient: {0}\".format(i+1))\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, checking this for the standardized data renders just about the same results, where it appears that the LDA model is picking up on the frequency components of the images that separate the classes the best. Given the small sample sizes per class, I suspect that LDA may be overfitting here. Which leads us to another experimental avenue next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_std = LinearDiscriminantAnalysis()\n",
    "X_std_train_lda = lda_std.fit_transform(X_std_train, y_std_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "for i in range(20):\n",
    "    eigenvector = lda_std.coef_[i].reshape(112, 92)\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(eigenvector, cmap='gray')\n",
    "    plt.title(\"Coefficient: {0}\".format(i+1))\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let us also fit an LDA model on the transformed training data acquired from our  first PCA model. We can see below that the LDA coefficients seem to be honeing onto the region of the image that most disittifies the class of the instance, that being the oblong area that contains the faces. It should be noted that proformaing the same method with the standardized data rendering nothing near as instreasing but instead for a few sparse dots, but is perhaps more or less functionally equivalent, but less pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_pca = LinearDiscriminantAnalysis()\n",
    "X_train_pca_lda = lda_pca.fit_transform(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "for i in range(20):\n",
    "    eigenvector = (pca.inverse_transform(lda_pca.coef_[i])).reshape(112, 92)\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(eigenvector, cmap='gray')\n",
    "    plt.title(\"Coefficient: {0}\".format(i+1))\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted our subspace models, let us now attempt to train a classifier on the projected training data. This we'll use the simple random forest classifier setup for each subspace combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc_std = RandomForestClassifier()\n",
    "\n",
    "rfc_pca = RandomForestClassifier()\n",
    "rfc_lda = RandomForestClassifier()\n",
    "rfc_lda_pca = RandomForestClassifier()\n",
    "\n",
    "rfc_std_pca = RandomForestClassifier()\n",
    "rfc_std_lda = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc_std.fit(X_std_train, y_std_train)\n",
    "\n",
    "rfc_pca.fit(X_train_pca, y_train)\n",
    "rfc_lda.fit(X_train_lda, y_train)\n",
    "rfc_lda_pca.fit(X_train_pca_lda, y_train)\n",
    "\n",
    "rfc_std_pca.fit(X_std_train_pca, y_std_train)\n",
    "rfc_std_lda.fit(X_std_train_lda, y_std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll transform the rest of our test data and predict using our list of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(X_test)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "X_test_pca_lda = lda_pca.transform(X_test_pca)\n",
    "\n",
    "X_std_test_pca = pca_std.transform(X_std_test)\n",
    "X_std_test_lda = lda_std.transform(X_std_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(X_test)\n",
    "y_std_pred = rfc_std.predict(X_std_test)\n",
    "\n",
    "y_pred_pca = rfc_pca.predict(X_test_pca)\n",
    "y_pred_lda = rfc_lda.predict(X_test_lda)\n",
    "y_pred_pca_lda = rfc_lda_pca.predict(X_test_pca_lda)\n",
    "\n",
    "y_std_pred_pca = rfc_std_pca.predict(X_std_test_pca)\n",
    "y_std_pred_lda = rfc_std_lda.predict(X_std_test_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = confusion_matrix(y_test,y_pred)\n",
    "accuracy_std = confusion_matrix(y_test,y_std_pred)\n",
    "\n",
    "accuracy_pca = confusion_matrix(y_test,y_pred_pca)\n",
    "accuracy_lda = confusion_matrix(y_test,y_pred_lda)\n",
    "accuracy_pca_lda = confusion_matrix(y_test,y_pred_pca_lda)\n",
    "\n",
    "accuracy_std_pca = confusion_matrix(y_std_test,y_std_pred_pca)\n",
    "accuracy_std_lda = confusion_matrix(y_std_test,y_std_pred_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, ax,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None):\n",
    "    ticks=np.linspace(0, 39,num=40)\n",
    "    ax.imshow(cm, interpolation='none', cmap=cmap)\n",
    "#     ax.colorbar()\n",
    "    plt.xticks(ticks,fontsize=6)\n",
    "    plt.yticks(ticks,fontsize=6)\n",
    "    ax.grid(True)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the confusion matrix over the number of classes to gain qualitative view the accuracy of the various approaches. First shown are the results for PCA with and without standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(accuracy_pca, ax, title='accuracy_pca')\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(accuracy_std_pca, ax, title='accuracy_std_pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, show here is the same case but for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(accuracy_lda, ax, title='accuracy_lda')\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(accuracy_std_lda, ax, title='accuracy_std_lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly is the base case where the data is directly feed to the classifier v.s. our convoluted method of chaining LDA via a prior PCA transform with final random forest classifier fitting with the resulting cascaded output. As we can see this last experimental methods panned out be rather dismal. Additionally the stock methods without any subspace intervention only did slightly worse that the non standardized LDA approach previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(accuracy, ax, title='accuracy (no subspace)')\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(accuracy_pca_lda, ax, title='accuracy_pca_lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "accuracy_std = accuracy_score(y_test,y_std_pred)\n",
    "\n",
    "accuracy_pca = accuracy_score(y_test,y_pred_pca)\n",
    "accuracy_lda = accuracy_score(y_test,y_pred_lda)\n",
    "accuracy_pca_lda = accuracy_score(y_test,y_pred_pca_lda)\n",
    "\n",
    "accuracy_std_pca = accuracy_score(y_std_test,y_std_pred_pca)\n",
    "accuracy_std_lda = accuracy_score(y_std_test,y_std_pred_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Latex, display\n",
    "import tabulate\n",
    "table = [[\"Approch\",\"Accuracy\"],\n",
    "         [\"RFC PCA\",accuracy_pca],\n",
    "         [\"RFC PCA std\",accuracy_std_pca],\n",
    "         [\"RFC LDA\",accuracy_lda],\n",
    "         [\"RFC LDA std\",accuracy_std_lda],\n",
    "         [\"RFC\",accuracy],\n",
    "         [\"RFC std\",accuracy_std],\n",
    "         [\"RFC LDA PCA\",accuracy_pca_lda],\n",
    "        ]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "display(Latex(tabulate.tabulate(table, tablefmt='latex')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In summary, we can see that in most cases, standardizing the data did nothing to help the classification performance, as anticipated given the data was already of the same unit of grayscale intensity. Additionally, RFC LDA appear to be approximately equivalent to just feeding the data directly to the random forest classifier. This could say a few things about the scenario. \n",
    "\n",
    "First, that data set is still quite small for modern PCs as opposed to systems back in 1994 when this att dataset was published, thus the bar or necessity for subspace methods has shifted dramatically further given that memory here is not much of an issue. Even with our LDA library using the pseudo inverse to handle the larger number of features vs instances, the majority of the descriptive power can still be ascertained the the RF classifier model. If the dimension of data was larger still, or the sparsity of samples in the feature space was worse, then this could warrant the use of feature space prepsoccessing.\n",
    "\n",
    "One thing not measured here was the training and evaluation time. Given that the subspace transformation used here does not have an equivalent number of eigenvectors as compared to the dimension of the original feature space, we know the transformation to be inherently lossy. This inevitably leads to reducing the reasoning power from the training data or inference during evaluation.  however if the size of the data set is too large, then reducing the computational complexity by reducing the dimension of the data  can be a suitable compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "> In robotics sound source localization is a frequent challenge. In the file http://www.hichristensen.com/demo-delay.wav estimate the delay between the two sound channels using FFT. provide an explanation of how you computes the delay between the two channels.\n",
    "\n",
    "> For both questions provide a description of the approach adopted, the associated code and a description of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the delay between two signals, we’re essentially searching for the phase offset between them. This can be done by checking the cross correlation of one signal with the other. More specifically, this can be done using the FFT by reformulating the convolution operation in the time domain into a single multiplication in the fourier domain. I.e. using the convolution theorem:\n",
    "\n",
    "$$\n",
    "\\mathcal{F}\\{f*g\\}=\\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}\n",
    "$$\n",
    "\n",
    "Thus, we can find the maximum response of the convolution between the two signals, i.e the counter shift rendering the signals in maximum alignment, by taking the argmax over the discrete inverse fourier transform of the product. From this we can determine $k$ in $k \\cdot dt$, where $dt$ is the sample period for the discrete signal, $k$ is the number of samples and thus $k \\cdot dt$ is the total phase shift in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some functions for reading in the WAV file, and calculating the forward and infevers FFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft\n",
    "from scipy.fftpack import ifft\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read in the file and split the two channels. From the stereo format of the file, we'll note the left channel to be the first, and the right channel to be the second, as is conventional in sound engineering and file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs, data = wavfile.read('data/demo-delay.wav')\n",
    "L = data.T[0]\n",
    "R = data.T[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From listening to the audio recording using a stereo headset, we can tell from the delay in arrival using our directional sense of hearing that the sound source is seems to originate from the left side from the perspective listener. If we take a look at the start of the signal closely, we can clearly see the subtle time shift with the right channel lagging behind the left.\n",
    "\n",
    "This makes intuitive sense and matches with our sense of direction given that the time of arrival for a sound source on our left side would sound sooner in our left ear than our right. This ability for sub microsecond phase shift detection is what enables most binaural animals to localize sound sources, often coupled with servoing of the head to more accurately gauge bearing when sensing the change in phase shift over time and orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(np.array_split(L, 500)[0], 'b', label='Left')\n",
    "ax.plot(np.array_split(R, 500)[0], 'r', label='Right')\n",
    "ax.set_xlabel('Samples (Discrete Time)')\n",
    "ax.set_ylabel('Amplitide (Microphone Signal)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing and comparing the audio samples, it is generally a good idea to remove the DC component, as we don't yet exactly know how else the two signals may differ. Here we'll do so by removing the mean average respectively from both channels.\n",
    "\n",
    "For microphone arrays separated by larger distances or some insulation that would otherwise result in greater signal attenuation in one recorded channel that the other, normalizing each channel by its standard deviation may also be a good idea. As it seems from the figure above that the two signals are of the exact same amplitude, the introduction of smaller floating point computation does not seem necessary in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = L - L.mean()\n",
    "# L = L/L.std()\n",
    "R = R - R.mean()\n",
    "# R = R/R.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use numpy to compute the N-dimensional discrete Fourier Transform for our real input signal. However, to ensure our fft computation is optimized for radix {2, 3, 4, 5}, we must find the next composite of the prime factors 2, 3, and 5 which are greater than or equal to target to pad with zeros, i.e. our maximum discrete sample size of our convolution operation, $|L| + |R| - 1$. This is also known as 5-smooth numbers, regular numbers, or Hamming numbers. We also compute the slice needed for later when shaving off the excess padding from our inverse operation.\n",
    "\n",
    "Source: https://github.com/scipy/scipy/pull/3144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.array(L.shape)\n",
    "s2 = np.array(R.shape)\n",
    "\n",
    "shape = s1 + s2 - 1\n",
    "fshape = [scipy.fftpack.helper.next_fast_len(int(d)) for d in shape]\n",
    "fslice = tuple([slice(0, int(sz)) for sz in shape])\n",
    "\n",
    "print(\"s1: \", s1)\n",
    "print(\"s2: \", s2)\n",
    "print(\"fshape: \", fshape)\n",
    "print(\"fslice: \", fslice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally compute the FFT for the left and right channels, we multiply them in the fourier domain, then recover the convolution by taking the reverse FFT of the product. Note that in order to compute the convolution correctly, we must first reverse the kernel, or the right channel in this case, before computing its FFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp1 = np.fft.rfftn(L, fshape)\n",
    "sp2 = np.fft.rfftn(np.flip(R, axis=0), fshape)\n",
    "ret = (np.fft.irfftn(sp1 * sp2, fshape)[fslice].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the response. As shown below, we see the response grow from zero to some nominal noise floor as the kernel convolves over the left channel. From the sharp spike in response just past 200k on the x axis, we can tell that the signals are indeed quite similar, as they best aling at a narrow range in phase offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(ret)\n",
    "# ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the response closer and see the residual harmonics in the sliding convictions, given the arbitrary waveform my happen to coincide to lesser extents at period lengths of other carrier frequencies of the audible voice sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = L.size\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(ret[nsamples-1000:nsamples+1000])\n",
    "# ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we use the sample frequency as declared in the metadata of the file, $44.1kHz$ in this case, to derive the sample period $dt$, we can multiple this with $k$ to determine the time shift. Note that we can't really speak of the phase angle, as we are working with arbitrary waveforms that are not composed of a single frequency, thus a single angle would not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.arange(1-nsamples, nsamples)\n",
    "recovered_sample_shift = dt[ret.argmax()]\n",
    "\n",
    "f = 44.1e3\n",
    "p = 1/f\n",
    "recovered_time_shift = recovered_sample_shift * p\n",
    "\n",
    "print(\"Recovered shift (samples): \", recovered_sample_shift)\n",
    "print(\"Recovered shift (seconds): \", recovered_time_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a simple sanity check by multiplying this time by the speed of sound, $343 m/s$  in ambient air temperature to estimate a 'distance' between the two phased array microphones. We find this to be around $\\sim17cm$, which is an approximately on the scale for human ear separation, thus why the acoustic delay sound naturally from the left for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Array Displacement\", 343*recovered_time_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check this by reversing the shift on the right channel as to realign with the left channel waveform. From the figure below, we see that $k=-22$ does indeed bring the two waveform back info perfect synchrony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(np.array_split(L[0:], 1000)[0][100:110],'bv-', label='Left')\n",
    "ax.plot(np.array_split(R[21:], 1000)[0][100:110],'g-', label='Right (-21)')\n",
    "ax.plot(np.array_split(R[23:], 1000)[0][100:110],'y-', label='Right (-23)')\n",
    "ax.plot(np.array_split(R[22:], 1000)[0][100:110],'ro--', label='Right (-22)')\n",
    "ax.set_xlabel('Samples (Discrete Time)')\n",
    "ax.set_ylabel('Amplitide (Microphone Signal)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
