{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 : Math for Robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Ruffin White  \n",
    "Course: CSE291  \n",
    "Date: Mar 9 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Make inline plots vector graphics instead of raster graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "# set_matplotlib_formats('png', 'pdf')\n",
    "# set_matplotlib_formats('png')\n",
    "\n",
    "# import modules for plotting and data analysis\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "\n",
    "> Use the ATT dataset http://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att faces.zip. to compute subspaces for the PCA and LDA methods. Provide illustration of the respective 1st, 2nd and 3rd eigenvectors. Compute the recognition rates for the test-set. Report\n",
    "\n",
    "* Correct classification\n",
    "* Incorrect classification\n",
    "\n",
    "> Provide at least one suggestion for how you might improve performance of the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "from collections import OrderedDict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_dir_path):\n",
    "    rows = []\n",
    "    height, width = 112, 92\n",
    "    flattened = height * width # 10304\n",
    "    X_keys = [\"X_\" + str(i) for i in range(flattened)]\n",
    "    paths = pathlib.Path(data_dir_path).glob('**/*.pgm')\n",
    "    for path in paths:\n",
    "        im = Image.open(str(path))\n",
    "        X_values = np.array(im).flatten()\n",
    "        y_value = path.parent.name\n",
    "        row = OrderedDict(zip(X_keys, X_values))\n",
    "        row['y'] = y_value\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir_path = 'data/att_faces/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = read_data(data_dir_path)\n",
    "X = df.drop('y',axis=1)\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "numbers = [0,1,398,399]\n",
    "for i in range(4):\n",
    "    im = X.loc[numbers[i]].values.reshape(112, 92)\n",
    "    label = y.loc[numbers[i]]\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(label)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.as_matrix(), y.as_matrix(),\n",
    "                                                    test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=X_train.shape[1])\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print('X_std_pca.shape:', X_train_pca.shape)\n",
    "print('pca.components_:', pca.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "for i in range(20):\n",
    "    eigenvector = pca.components_[i].reshape(112, 92)\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(eigenvector, cmap='gray')\n",
    "    plt.title(\"Eigenvector: {0}\".format(i+1))\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "> In robotics sound source localization is a frequent challenge. In the file http://www.hichristensen.com/demo-delay.wav estimate the delay between the two sound channels using FFT. provide an explanation of how you computes the delay between the two channels.\n",
    "\n",
    "> For both questions provide a description of the approach adopted, the associated code and a description of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the delay between two signals, weâ€™re essentially searching for the phase offset between them. This can be done by checking the cross correlation, or convolution one signal with the other. More specifically, this can be done using the FFT by reformulating the convolution operation in the time domain into a single multiplication in the fourier domain. I.e. using the convolution theorem:\n",
    "\n",
    "$$\n",
    "\\mathcal{F}\\{f*g\\}=\\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}\n",
    "$$\n",
    "\n",
    "Thus, we can find the maximum response of the convolution between the two signals, i.e the counter shift rendering the signals in maximum alignment, by taking the argmax over the discrete inverse fourier transform of the product. From this we can determine $k$ in $k \\cdot dt$, where $dt$ is the sample period for the discrete signal, $k$ is the number of samples and thus $k \\cdot dt$ is the total phase shift in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some functions for reading in the WAV file, and calculating the forward and infevers FFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft\n",
    "from scipy.fftpack import ifft\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read in the file and split the two channels. From the stereo format of the file, we'll note the left channel to be the first, and the right channel to be the second, as is conventional in sound engineering and file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs, data = wavfile.read('data/demo-delay.wav')\n",
    "L = data.T[0]\n",
    "R = data.T[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From listening to the audio recording using a stereo headset, we can tell from the delay in arrival using our directional sense of hearing that the sound source is seems to originate from the left side from the perspective listener. If we take a look at the start of the signal closely, we can clearly see the subtle time shift with the right channel lagging behind the left.\n",
    "\n",
    "This makes intuitive sense and matches with our sense of direction given that the time of arrival for a sound source on our left side would sound sooner in our left ear than our right. This ability for sub microsecond phase shift detection is what enables most binaural animals to localize sound sources, often coupled with servoing of the head to more accurately gauge bearing when sensing the change in phase shift over time and orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(np.array_split(L, 500)[0], 'b', label='Left')\n",
    "ax.plot(np.array_split(R, 500)[0], 'r', label='Right')\n",
    "ax.set_xlabel('Samples (Discrete Time)')\n",
    "ax.set_ylabel('Amplitide (Microphone Signal)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing and comparing the audio samples, it is generally a good idea to remove the DC component, as we don't yet exactly know how else the two signals may differ. Here we'll do so by removing the mean average respectively from both channels.\n",
    "\n",
    "For microphone arrays separated by larger distances or some insulation that would otherwise result in greater signal attenuation in one recorded channel that the other, normalizing each channel by its standard deviation may also be a good idea. As it seem from the figure above that the two signals are of the exact same amplitude, the introduction of smaller floating point computation does not seem necessary in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = L - L.mean()\n",
    "# L = L/L.std()\n",
    "R = R - R.mean()\n",
    "# R = R/R.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use numpy to compute the N-dimensional discrete Fourier Transform for our real input signal. However, to ensure our fft computation is optimized for radix {2, 3, 4, 5}, we must find the next composite of the prime factors 2, 3, and 5 which are greater than or equal to target to pad with zeros, i.e. our maximum discrete sample size of our convolution operation, $|L| + |R| - 1$. This is also known as 5-smooth numbers, regular numbers, or Hamming numbers. We also compute the slice needed for later when shaving off the excess padding from our inverse operation.\n",
    "\n",
    "Source: https://github.com/scipy/scipy/pull/3144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.array(L.shape)\n",
    "s2 = np.array(R.shape)\n",
    "\n",
    "shape = s1 + s2 - 1\n",
    "fshape = [scipy.fftpack.helper.next_fast_len(int(d)) for d in shape]\n",
    "fslice = tuple([slice(0, int(sz)) for sz in shape])\n",
    "\n",
    "print(\"s1: \", s1)\n",
    "print(\"s2: \", s2)\n",
    "print(\"fshape: \", fshape)\n",
    "print(\"fslice: \", fslice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally compute the FFT for the left and right channels, multiply them in the fourier domain, then recover the convolution by taking the reverse FFT of the product. Note that in order to compute the convolution correctly, we must first reverse the kernel, or the right channel in this case, before computing its FFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp1 = np.fft.rfftn(L, fshape)\n",
    "sp2 = np.fft.rfftn(np.flip(R, axis=0), fshape)\n",
    "ret = (np.fft.irfftn(sp1 * sp2, fshape)[fslice].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the response. As shown below, we see the response grow from zero to some nominal noise floor as the kernel convolves over the left channel. From the sharp spike in response just past 200k on the x axis, we can tell that the signals are indeed quite similar, as they best aling at a narrow range in phase offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(ret)\n",
    "# ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the response closer and see the residual harmonics in the sliding convictions, given the arbitrary waveform my happen to coincide to lesser extents at period lengths of other carrier frequencies of the audible voice sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = L.size\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(ret[nsamples-1000:nsamples+1000])\n",
    "# ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we use the sample frequency as declared in the metadata of the file, $44.1kHz$ in this case, to derive the sample period $dt$, we can multiple this with $k$ to determine the time shift. Note that we can't really speak of the phase angle, as we are working with arbitrary waveforms that are not composed of a single frequency, thus a single angle would not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.arange(1-nsamples, nsamples)\n",
    "recovered_sample_shift = dt[ret.argmax()]\n",
    "\n",
    "f = 44.1e3\n",
    "p = 1/f\n",
    "recovered_time_shift = recovered_sample_shift * p\n",
    "\n",
    "print(\"Recovered shift (samples): \", recovered_sample_shift)\n",
    "print(\"Recovered shift (seconds): \", recovered_time_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a simple sanity check by multiplying this time by the speed of sound, $343 m/s$  in ambient air temperature to square a 'distance' between the two phased array microphones. We find this to be around $\\sim17cm$, which is an approximately on the scale for human ear separation, thus why the acoustic delay sound naturally from the left for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Array Displacement\", 343*recovered_time_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check this by reversing the shift on the right channel as to realign with the left channel waveform. From the figure below, we see that $k=-22$ does indeed bring the two waveform back info perfect synchrony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(np.array_split(L[0:], 1000)[0][100:110],'bv-', label='Left')\n",
    "ax.plot(np.array_split(R[21:], 1000)[0][100:110],'g-', label='Right (-21)')\n",
    "ax.plot(np.array_split(R[23:], 1000)[0][100:110],'y-', label='Right (-23)')\n",
    "ax.plot(np.array_split(R[22:], 1000)[0][100:110],'ro--', label='Right (-22)')\n",
    "ax.set_xlabel('Samples (Discrete Time)')\n",
    "ax.set_ylabel('Amplitide (Microphone Signal)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
