{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HW5 : Math for Robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Ruffin White  \n",
    "Course: CSE291  \n",
    "Date: Mar 23 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Make inline plots vector graphics instead of raster graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('pdf', 'svg')\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "# set_matplotlib_formats('png')\n",
    "\n",
    "# import modules for plotting and data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from log_progress import log_progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. \n",
    "\n",
    "> The world model is shown in figure 1. The robot is a differential drive\n",
    "system of size 50x50.\n",
    "\n",
    "1. Generate the configuration space for the robot with a grid size of 2x2 and 5 deg in angular resolution. Generate an illustration of what the configuration space looks like with the robot at orientations 0, 45 and 90 deg.\n",
    "2. Use greedy search find the shortest path between start-point (50,50) and end-point (750,250). Illustrate the path and provide its length.\n",
    "3. Compute the safest past from start to finish (hint: medial axis transform). Illustrate the path and provide its length.\n",
    "4. Use probabilistic roadmaps (PRM) to compute a path between startand end-points with 50, 100 and 500 sample points. What is the difference in path length? Illustrate each computed path.\n",
    "5. Do the same with Rapid exploring random trees (RRT). What are the main differences in performance between PRM and RRT? Illustrate each path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import scipy.spatial\n",
    "import scipy.signal\n",
    "\n",
    "from matplotlib import animation\n",
    "from collections import OrderedDict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a grid-based environment to encapsulate a discretized representation of our world model. We then will populate the 2D grid environment with the set number of wall objects that constrain possible paths what limits our work space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1\n",
    "\n",
    "width = int(800/scale)\n",
    "hight = int(300/scale)\n",
    "seg1 = int(200/scale)\n",
    "seg2 = int(600/scale)\n",
    "seg3 = int(400/scale)\n",
    "seg4 = int(100/scale)\n",
    "\n",
    "map_2d = np.zeros(shape=(width, hight))\n",
    "map_2d[0,:] = 1\n",
    "map_2d[:,0] = 1\n",
    "map_2d[:,hight-1] = 1\n",
    "map_2d[width-1,:] = 1\n",
    "\n",
    "map_2d[seg1-1,0:seg1] = 1\n",
    "map_2d[seg2-1,0:seg1] = 1\n",
    "\n",
    "map_2d[seg3-1,hight-1-seg1:hight-1] = 1\n",
    "map_2d[seg3-1-seg4:seg3-1+seg4,hight-1-seg1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well also define our Robot as pixelated shape, subject to the same scale used in deriving the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwidth = int(50/scale)\n",
    "rhight = int(50/scale)\n",
    "robot_2d = np.ones(shape=(rwidth, rhight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we’ll define a function to rotate our robot representation given an angle in degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_bound(image, angle):\n",
    "    # borrowed from https://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/\n",
    "    # grab the dimensions of the image and then determine the\n",
    "    # center\n",
    "    (h, w) = image.shape[:2]\n",
    "    (cX, cY) = (w // 2, h // 2)\n",
    " \n",
    "    # grab the rotation matrix (applying the negative of the\n",
    "    # angle to rotate clockwise), then grab the sine and cosine\n",
    "    # (i.e., the rotation components of the matrix)\n",
    "    M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
    "    cos = np.abs(M[0, 0])\n",
    "    sin = np.abs(M[0, 1])\n",
    " \n",
    "    # compute the new bounding dimensions of the image\n",
    "    nW = int((h * sin) + (w * cos))\n",
    "    nH = int((h * cos) + (w * sin))\n",
    " \n",
    "    # adjust the rotation matrix to take into account translation\n",
    "    M[0, 2] += (nW / 2) - cX\n",
    "    M[1, 2] += (nH / 2) - cY\n",
    " \n",
    "    # perform the actual rotation and return the image\n",
    "    return cv2.warpAffine(image, M, (nW, nH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given both are world and robot representations I represented as occupancy and 2D arrays, we can use simple 2d convolution, optimized with the FFT  transform,  to generate our configuration space with respect to a given angle, or list of  angles In which the robot is rotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_2d(map_2d, robot_2d_rot):\n",
    "    config_2d = scipy.signal.fftconvolve(map_2d, robot_2d_rot, mode='same')\n",
    "    return config_2d\n",
    "\n",
    "def get_config_space(map_2d, robot_2d_rot, angles):\n",
    "    config_space = np.empty((width, hight, angle_res))\n",
    "    for i, angle in enumerate(angles):\n",
    "        robot_2d_rot = rotate_bound(robot_2d, angle)\n",
    "        config_space[:,:,i] = get_config_2d(map_2d, robot_2d_rot)\n",
    "    return config_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a simple function to plot our workspace or partial configuration space for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map_2d(map_2d):\n",
    "    f, axs = plt.subplots(1,1,figsize=(24,64))\n",
    "#     axs.imshow(map_2d.T, cmap=plt.cm.Spectral_r, interpolation='none')\n",
    "    axs.imshow(map_2d.T, cmap=plt.cm.nipy_spectral, interpolation='none')\n",
    "    for d in [\"left\", \"top\", \"bottom\", \"right\"]:\n",
    "        axs.spines[d].set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here will generate our configuration space for the angles between 0 and 90 degrees at a degree resolution of one. Because we are using scipy’s fftconvolve function that padds our signal to next nearest power of the hamming numbers for the fft function optimized for those radixs. This makes this step quite faster than compared to traditional 2D convolution of signal/kernel or workspace/robot respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_res = 90\n",
    "angles = np.linspace(start=0, stop=90, num=angle_res, endpoint=False)\n",
    "config_space = get_config_space(map_2d, robot_2d, angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting this grid based world, we can see the layout as shown in the assignment figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map_2d(map_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next three figures, we can visualize the configuration space 4 robot rotations at 0, 45, and 90 degrees respectively. Because our configuration space was acquired using to the convolutions, we can visualize the raw response to provide additional insight in robot collisional intensity, as well as a regionalized cost map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_2d_rot = rotate_bound(robot_2d, 0)\n",
    "config_2d = get_config_2d(map_2d, robot_2d_rot)\n",
    "# config_2d = np.clip(a=config_2d, a_min=0, a_max=1)\n",
    "plot_map_2d(config_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_2d_rot = rotate_bound(robot_2d, 45)\n",
    "config_2d = get_config_2d(map_2d, robot_2d_rot)\n",
    "# config_2d = np.clip(a=config_2d, a_min=0, a_max=1)\n",
    "plot_map_2d(config_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_2d_rot = rotate_bound(robot_2d, 90)\n",
    "config_2d = get_config_2d(map_2d, robot_2d_rot)\n",
    "# config_2d = np.clip(a=config_2d, a_min=0, a_max=1)\n",
    "plot_map_2d(config_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our robot is rotationally symmetrical across every half pi radians, we can flatten the configuration space across rotations from 0 to 90 degrees by taking the max value along the 3rd axis on our configuration space 3D array representation. This renders the most conservative configuration space where the black region denote all points in the world reference frame that our robot can freely rotate in place.\n",
    "\n",
    "This is useful for robotic navigation stacks that commonly make use of rotational recovery behaviors when losing localization or determining alternate routes. The astute reader will notice that this is the same result as in taking the  the 2D deconvolution of the world map  with respect to a kernel that is a  simple Circle with a radius defined as the maximum distance a collision point along the robot geometry is with respect to the center of its rotation (assuming our differential drive model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space_2d = config_space.max(axis=2)\n",
    "# config_space_2d = np.clip(a=config_space_2d, a_min=0, a_max=0.001)\n",
    "plot_map_2d(config_space_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space_2d = config_space.max(axis=2)\n",
    "config_space_2d = np.clip(a=config_space_2d, a_min=0, a_max=0.001)\n",
    "plot_map_2d(config_space_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function to sweep through the configuration space in an animated fashion. However this is quite slow using to matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('GTKAgg')\n",
    "\n",
    "def plot_3d(config_space, animated=False, stride=1):\n",
    "#     config_space = config_space[::stride]\n",
    "    \n",
    "    fig, axs = plt.subplots(1,1,figsize=(16,6))\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "\n",
    "    def init():\n",
    "        axs.imshow(config_space[:,:,0].T, cmap=plt.cm.Spectral_r, interpolation='none')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "#         for d in [\"left\", \"top\", \"bottom\", \"right\"]:\n",
    "#             axs.spines[d].set_visible(False)\n",
    "        \n",
    "        return fig,\n",
    "    \n",
    "    def animate(i):\n",
    "        axs.imshow(config_space[:,:,i].T, cmap=plt.cm.Spectral_r, interpolation='none')\n",
    "        return fig,\n",
    "    \n",
    "    if animated:\n",
    "        frames = log_progress(range(angle_res), 1)\n",
    "        anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                       frames=frames, interval=20, blit=True)\n",
    "        video = anim.to_html5_video()\n",
    "        return video\n",
    "    else:\n",
    "        init()\n",
    "        return fig\n",
    "\n",
    "# empty2_video = plot_3d(config_space, animated=True)\n",
    "# display(HTML(empty2_video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better alternative is perhaps using pygtgraph and let OpenGL do the heavy lifting for us in rendering the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from pyqtgraph.Qt import QtCore, QtGui\n",
    "# import pyqtgraph as pg\n",
    "\n",
    "# # Interpret image data as row-major instead of col-major\n",
    "# pg.setConfigOptions(imageAxisOrder='row-major')\n",
    "\n",
    "# app = QtGui.QApplication([])\n",
    "\n",
    "# ## Create window with ImageView widget\n",
    "# win = QtGui.QMainWindow()\n",
    "# win.resize(800,300)\n",
    "# imv = pg.ImageView()\n",
    "# win.setCentralWidget(imv)\n",
    "# win.show()\n",
    "# win.setWindowTitle('pyqtgraph example: ImageView')\n",
    "\n",
    "# data = config_space[:,:,:]\n",
    "# # data = np.clip(a=data, a_min=0, a_max=1)\n",
    "\n",
    "\n",
    "# ## Display the data and assign each frame a time value from 1.0 to 3.0\n",
    "# imv.setImage(data.T, xvals=angles)\n",
    "\n",
    "# # Get the colormap\n",
    "# colormap = plt.cm.nipy_spectral  # cm.get_cmap(\"CMRmap\")\n",
    "# colormap._init()\n",
    "# lut = (colormap._lut * 255).view(np.ndarray)  # Convert matplotlib colormap from 0-1 to 0 -255 for Qt\n",
    "# imv.imageItem.setLookupTable(lut)\n",
    "\n",
    "# ptr = 0\n",
    "# def update():\n",
    "#     global data, ptr\n",
    "#     imv.imageItem.updateImage(data[:,:,ptr].T)\n",
    "#     imv.imageItem.setLookupTable(lut)\n",
    "#     ptr += 1\n",
    "#     ptr = ptr % angles.size\n",
    "# timer = QtCore.QTimer()\n",
    "# timer.timeout.connect(update)\n",
    "\n",
    "# ## Start Qt event loop unless running in interactive mode.\n",
    "# if __name__ == '__main__':\n",
    "#     import sys\n",
    "#     if (sys.flags.interactive != 1) or not hasattr(QtCore, 'PYQT_VERSION'):\n",
    "#         timer.start(50)\n",
    "#         QtGui.QApplication.instance().exec_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use pygtgraph to slice the Configuration space arbitrarily, similarly done with MRI scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from pyqtgraph.Qt import QtCore, QtGui\n",
    "# import pyqtgraph as pg\n",
    "\n",
    "# app = QtGui.QApplication([])\n",
    "\n",
    "# ## Create window with two ImageView widgets\n",
    "# win = QtGui.QMainWindow()\n",
    "# win.resize(800,800)\n",
    "# win.setWindowTitle('pyqtgraph example: DataSlicing')\n",
    "# cw = QtGui.QWidget()\n",
    "# win.setCentralWidget(cw)\n",
    "# l = QtGui.QGridLayout()\n",
    "# cw.setLayout(l)\n",
    "# imv1 = pg.ImageView()\n",
    "# imv2 = pg.ImageView()\n",
    "# l.addWidget(imv1, 0, 0)\n",
    "# l.addWidget(imv2, 1, 0)\n",
    "# win.show()\n",
    "\n",
    "# roi = pg.LineSegmentROI([[10, 64], [120,64]], pen='r')\n",
    "# imv1.addItem(roi)\n",
    "\n",
    "# x1 = np.linspace(-30, 10, 128)[:, np.newaxis, np.newaxis]\n",
    "# x2 = np.linspace(-20, 20, 128)[:, np.newaxis, np.newaxis]\n",
    "# y = np.linspace(-30, 10, 128)[np.newaxis, :, np.newaxis]\n",
    "# z = np.linspace(-20, 20, 128)[np.newaxis, np.newaxis, :]\n",
    "# d1 = np.sqrt(x1**2 + y**2 + z**2)\n",
    "# d2 = 2*np.sqrt(x1[::-1]**2 + y**2 + z**2)\n",
    "# d3 = 4*np.sqrt(x2**2 + y[:,::-1]**2 + z**2)\n",
    "# # data = (np.sin(d1) / d1**2) + (np.sin(d2) / d2**2) + (np.sin(d3) / d3**2)\n",
    "# data = config_space[:,:,:]\n",
    "\n",
    "# def update():\n",
    "#     global data, imv1, imv2\n",
    "#     d2 = roi.getArrayRegion(data, imv1.imageItem, axes=(1,2))\n",
    "#     imv2.setImage(d2)\n",
    "    \n",
    "# roi.sigRegionChanged.connect(update)\n",
    "\n",
    "\n",
    "# ## Display the data\n",
    "# imv1.setImage(data)\n",
    "# imv1.setHistogramRange(-0.01, 0.01)\n",
    "# imv1.setLevels(-0.003, 0.003)\n",
    "\n",
    "# update()\n",
    "\n",
    "# ## Start Qt event loop unless running in interactive mode.\n",
    "# if __name__ == '__main__':\n",
    "#     import sys\n",
    "#     if (sys.flags.interactive != 1) or not hasattr(QtCore, 'PYQT_VERSION'):\n",
    "#         QtGui.QApplication.instance().exec_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we can export the occupancy grid from our configuration workspace and to a three dimensional Point cloud. Obviously this only works because of the 2D dimension of our initial world frame with the additional dimension of rotation. However the same slicing technique, as above, could be used to introspect the configuration space for higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plyfile import PlyData, PlyElement\n",
    "# data = np.clip(a=config_space, a_min=0, a_max=1)\n",
    "# points = np.asarray(np.where(data == 1)).T\n",
    "# vertex = np.array(list(map(tuple, points)), dtype=[('x', 'u2'), ('y', 'u2'), ('z', 'u2')])\n",
    "# el = PlyElement.describe(vertex, 'config_space')\n",
    "# PlyData([el]).write('config_space.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "# import yaml\n",
    "# matrix = np.random.randint(2, size=(10,7))\n",
    "# with open('foo.yaml', 'w') as f:\n",
    "#     yaml.dump(config_space.flatten().tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh = 0.001\n",
    "# config_space_bool = config_space.copy()\n",
    "# config_space_bool[config_space < thresh] = 0\n",
    "# config_space_bool[config_space >= thresh] = 1\n",
    "# plot_map_2d(config_space_bool[:,:,45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('config_space_bool.yaml', config_space_bool.flatten(), delimiter=',', fmt='%d,')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the safest is path through our workspace environment will in this case equate as being the trajectory that is  maximally distant from all nearby obstacles. Essentially this necessitates computing the voronoi partitions for the empty regions in the work space. We can then travers the set of points generated from this tessellation as it provides us a simplistic roadmap. We then need only find the closest decomposed points to both the start and the goal. As a final check, we can sample the resulting trajectory using our configuration space as a collisions look up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.asarray(np.where(map_2d == 1)).T\n",
    "vor = scipy.spatial.Voronoi(points)\n",
    "\n",
    "vertices = vor.vertices[::1].T.astype(int)\n",
    "map_2d_vert = map_2d.copy()\n",
    "map_2d_vert[vertices[0],vertices[1]] = 2\n",
    "\n",
    "plot_map_2d(map_2d_vert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "221px",
    "left": "1326.89px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
